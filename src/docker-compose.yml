version: "3.8"
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: ERROR
      ZOOKEEPER_TOOLS_LOG4J_LOGLEVEL: ERROR
  kafka:
    image: confluentinc/cp-kafka
    ports:
      - 29092:29092
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG4J_ROOT_LOGLEVEL: ERROR
      KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR
  sql-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    ports:
      - 1433:1433
    environment:
      ACCEPT_EULA: Y
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
  # The only purpose of this service is to wait for the SQL Server to start and create the DB and Table
  sql-setup:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      MSSQL_HOST: sql-server
      MSSQL_SA_USER: SA
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
      DB_NAME: ${DB_NAME}
    volumes:
      - ./db:/db
    working_dir: /db
    depends_on:
      - sql-server
    entrypoint: "./init-db.sh"
  build-wheel:
    image: python:3-bullseye
    volumes:
      - ./common_lib:/common_lib
      - shared_volume:/common_lib/dist
    # Explicitly delete Wheel before building in order to avoid picking up existing version from mount
    entrypoint: ["bash", "-c", "rm -rf /common_lib/dist/*.whl && pip install build && python -m build /common_lib"]
  spark:
    image: apache/spark-py:v3.4.0
    ports:
      - 4040:4040
    volumes:
      - ./pyspark_app:/app
      - shared_volume:/common_lib
      - ./metadata:/metadata
    depends_on:
      build-wheel:
        condition: service_completed_successfully
      kafka:
        condition: service_started
      sql-setup:
        condition: service_completed_successfully
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      MSSQL_HOST: sql-server
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
      DB_NAME: ${DB_NAME}
    # There should be no whitespace between the comma separated packages https://stackoverflow.com/questions/33928029/how-to-specify-multiple-dependencies-using-packages-for-spark-submit
    # The mssql-jdbc package must be explicitly added https://stackoverflow.com/questions/66903523/apache-spark-connector-for-sql-server-and-azure-sql
    # The default Ivy Cache location is not writable in the container, hence we have to override it to /tmp https://stackoverflow.com/a/69559038
    command: ["/opt/spark/bin/spark-submit", "--packages",
      "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,com.microsoft.azure:spark-mssql-connector_2.12:1.3.0-BETA,com.microsoft.sqlserver:mssql-jdbc:12.4.0.jre8",
      "--conf", "spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp",
      "--conf", "spark.ui.prometheus.enabled=true",
      "--conf", "spark.executor.processTreeMetrics.enabled=true",
      "--conf", "spark.metrics.conf.*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet",
      "--conf", "spark.metrics.conf.*.sink.prometheusServlet.path=/metrics/prometheus",
      "--conf", "spark.metrics.namespace=MyPySparkStreamingApp",
      "--conf", "spark.metrics.appStatusSource.enabled=true",
      "--py-files", "/common_lib/*.whl", "/app/main.py"]
  iot-telemetry-simulator:
    image: mcr.microsoft.com/oss/azure-samples/azureiot-telemetrysimulator
    depends_on:
      - spark
    environment:
      KafkaConnectionProperties: '{"bootstrap.servers": "kafka:9092"}'
      KafkaTopic: ${KAFKA_TOPIC} #Topic will be created with default settings if it doesn't exist
      DeviceCount: 3
      MessageCount: 0 # send unlimited
      Template: '{"deviceId": "$.DeviceId", "deviceTimestamp": "$.Time", "doubleValue": $.DoubleValue}'
      Variables: '[{"name": "DoubleValue", "randomDouble":true, "min":0.22, "max":1.25}]'
  prometheus:
    image: prom/prometheus
    depends_on:
      - spark
    volumes:
      - ./prometheus/:/etc/prometheus/
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    ports:
      - 9090:9090
  grafana:
    image: grafana/grafana
    depends_on:
      - prometheus
    volumes:
      - ./grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
      - ./grafana/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml
      - ./grafana/dashboards/:/var/lib/grafana/dashboards/
    ports:
      - 3000:3000
volumes:
  shared_volume:
